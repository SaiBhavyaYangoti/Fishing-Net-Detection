# -*- coding: utf-8 -*-
"""fishing_net_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-NJwikjsTLlDvZqxM0x-S9NCcrJyOa4s
"""

#importing libraries
import pandas as pd
from sklearn.model_selection import train_test_split, learning_curve, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
from sklearn.metrics import f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

#loading and displaying top 5 rows from the dataset
df=pd.read_csv("/content/drive/MyDrive/Datasets/network attack/Train_Data.csv")
df.head()

#printing column names
df.columns

#checking for null values
df.isna().sum()

# Separate features and target
X = df.drop(columns=['attack'])
y = df['attack']

# Identify categorical and numerical columns
categorical_cols = ['protocoltype', 'flag', 'service']
numerical_cols = X.columns.difference(categorical_cols)

# Preprocessing pipeline for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)])

# Fit the preprocessor and transform the data
X_transformed = preprocessor.fit_transform(X)

# Get the feature names after transformation
num_feature_names = numerical_cols.tolist()
cat_feature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)
all_feature_names = num_feature_names + cat_feature_names.tolist()

# Convert the transformed data back into a DataFrame
X_transformed_df = pd.DataFrame(X_transformed, columns=all_feature_names)

# Display the transformed data
print(X_transformed_df.head())

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model pipeline with LogisticRegression
model = Pipeline(steps=[('preprocessor', preprocessor),
                        ('classifier', LogisticRegression(random_state=42, max_iter=1000))])

# Train the model
model.fit(X_train, y_train)

# Predict on the training set
y_train_pred = model.predict(X_train)

# Predict on the validation set
y_val_pred = model.predict(X_val)

# Calculate F1 scores
f1_train = f1_score(y_train, y_train_pred, average='weighted')
f1_val = f1_score(y_val, y_val_pred, average='weighted')

print(f'Training F1 Score: {f1_train}')
print(f'Validation F1 Score: {f1_val}')

if f1_val > f1_train:
  print("Model might be overfitting")
else:
  print("Model generalizes well")

# Load the test dataset
test_data = pd.read_csv('/content/drive/MyDrive/Datasets/network attack/Test_Data.csv')

# Predict the target column for the test data
y_test_pred = model.predict(test_data)

# Create a DataFrame for the predictions
test_predictions = pd.DataFrame({'attack': y_test_pred})

# Save the predictions to a CSV file
test_predictions.to_csv('/content/drive/MyDrive/Datasets/network attack/Test_Predictions.csv', index=False)

print('Predictions saved to Test_Predictions.csv')